{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bc6c4e4a3557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib.data import Dataset\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework.ops import convert_to_tensor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "from tensorflow.contrib.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folders=os.listdir('/home/ubuntu/office31/')\n",
    "datas={'amazon':'/home/ubuntu/office31/amazon/images/',\n",
    "       'webcam':'/home/ubuntu/office31/webcam/images/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataGenerator(object):\n",
    "    \n",
    "\n",
    "    def __init__(self, case, mode, batch_size, num_classes, shuffle=True,\n",
    "                 buffer_size=1000):\n",
    "       \n",
    "        self.case = case\n",
    "        self.num_classes = num_classes\n",
    "        self._read_txt_file()        \n",
    "        self.data_size = len(self.labels)\n",
    "       \n",
    "        if shuffle:\n",
    "            self._shuffle_lists()\n",
    "        \n",
    "        self.img_paths = convert_to_tensor(self.img_paths, dtype=dtypes.string)\n",
    "        self.labels = convert_to_tensor(self.labels, dtype=dtypes.int32)\n",
    "\n",
    "        data = Dataset.from_tensor_slices((self.img_paths, self.labels))\n",
    "\n",
    "        \n",
    "        if mode == 'training':\n",
    "            data = data.map(self._parse_function_train, num_threads=8,\n",
    "                      output_buffer_size=100*batch_size)\n",
    "\n",
    "        elif mode == 'inference':\n",
    "            data = data.map(self._parse_function_inference, num_threads=8,\n",
    "                      output_buffer_size=100*batch_size)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode '%s'.\" % (mode))\n",
    "\n",
    "        \n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "        \n",
    "        data = data.batch(batch_size)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def _read_txt_file(self):\n",
    "        \n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        folders=os.listdir(datas[self.case])\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(folders)\n",
    "        print(integer_encoded,folders)\n",
    "        for f in range(len(folders)):\n",
    "            images=os.listdir(datas[self.case]+folders[f]+'/')\n",
    "            for i in range(len(images)):\n",
    "                img_path=datas[self.case]+folders[f]+'/'+images[i]\n",
    "                self.img_paths.append(img_path)\n",
    "                self.labels.append(integer_encoded[f])\n",
    "                \n",
    "       \n",
    "    def _shuffle_lists(self):\n",
    "      \n",
    "        path = self.img_paths\n",
    "        labels = self.labels\n",
    "        permutation = np.random.permutation(self.data_size)\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        for i in permutation:\n",
    "            self.img_paths.append(path[i])\n",
    "            self.labels.append(labels[i])\n",
    "\n",
    "    def _parse_function_train(self, filename, label):\n",
    "        \n",
    "        # convert label number into one-hot-encoding\n",
    "        one_hot = tf.one_hot(label, self.num_classes)\n",
    "\n",
    "        \n",
    "        img_string = tf.read_file(filename)\n",
    "        img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "        img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
    "       \n",
    "        img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
    "\n",
    "        # RGB -> BGR\n",
    "        img_bgr = img_centered[:, :, ::-1]\n",
    "\n",
    "        return img_bgr, one_hot\n",
    "\n",
    "    def _parse_function_inference(self, filename, label):\n",
    "       \n",
    "        one_hot = tf.one_hot(label, self.num_classes)\n",
    "\n",
    "       \n",
    "        img_string = tf.read_file(filename)\n",
    "        img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "        img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
    "        img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
    "\n",
    "       \n",
    "        img_bgr = img_centered[:, :, ::-1]\n",
    "\n",
    "        return img_bgr, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 17  2 15 10 11 25  9 19 13 27  0 16  6 12  4 22 18  3 28 21  5  1 24\n",
      " 30  8 23 26  7 20 14] ['tape_dispenser', 'mug', 'bike_helmet', 'monitor', 'headphones', 'keyboard', 'ruler', 'file_cabinet', 'pen', 'letter_tray', 'speaker', 'back_pack', 'mouse', 'desk_chair', 'laptop_computer', 'bottle', 'projector', 'paper_notebook', 'bookcase', 'stapler', 'printer', 'calculator', 'bike', 'ring_binder', 'trash_can', 'desktop_computer', 'punchers', 'scissors', 'desk_lamp', 'phone', 'mobile_phone']\n"
     ]
    }
   ],
   "source": [
    "tr_data = ImageDataGenerator('amazon',\n",
    "                                 mode='training',\n",
    "                                 batch_size=200,\n",
    "                                 num_classes=31,\n",
    "                                 shuffle=True)\n",
    "iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                       tr_data.data.output_shapes)\n",
    "img,label = iterator.get_next()\n",
    "\n",
    "\n",
    "training_init_op = iterator.make_initializer(tr_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(object):\n",
    "    \"\"\"Implementation of the AlexNet.\"\"\"\n",
    "\n",
    "    def __init__(self, x, keep_prob, num_classes, skip_layer,\n",
    "                 weights_path='DEFAULT'):\n",
    "        \n",
    "        self.X = x\n",
    "        self.NUM_CLASSES = num_classes\n",
    "        self.KEEP_PROB = keep_prob\n",
    "        self.SKIP_LAYER = skip_layer\n",
    "\n",
    "        if weights_path == 'DEFAULT':\n",
    "            self.WEIGHTS_PATH = '/home/ubuntu/bvlc_alexnet.npy'\n",
    "        else:\n",
    "            self.WEIGHTS_PATH = weights_path\n",
    "\n",
    "        # Call the create function to build the computational graph of AlexNet\n",
    "        self.create()\n",
    "\n",
    "    def create(self):\n",
    "       \n",
    "        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
    "        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "        norm1 = lrn(conv1, 2, 1e-05, 0.75, name='norm1')\n",
    "        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "        \n",
    "        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n",
    "        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\n",
    "        norm2 = lrn(conv2, 2, 1e-05, 0.75, name='norm2')\n",
    "        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "        \n",
    "        # 3rd Layer: Conv (w ReLu)\n",
    "        conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\n",
    "\n",
    "        # 4th Layer: Conv (w ReLu) splitted into two groups\n",
    "        conv4 = conv(conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\n",
    "\n",
    "        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\n",
    "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "\n",
    "        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "        flattened = tf.reshape(pool5, [-1, 6*6*256])\n",
    "        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n",
    "        dropout6 = dropout(fc6, self.KEEP_PROB)\n",
    "\n",
    "        # 7th Layer: FC (w ReLu) -> Dropout\n",
    "        fc7 = fc(dropout6, 4096, 4096, name='fc7')\n",
    "        dropout7 = dropout(fc7, self.KEEP_PROB)\n",
    "\n",
    "        # 8th Layer: FC and return unscaled activations\n",
    "        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu=False, name='fc8')\n",
    "\n",
    "    def load_initial_weights(self, session):\n",
    "        \n",
    "        # Load the weights into memory\n",
    "        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()\n",
    "\n",
    "        # Loop over all layer names stored in the weights dict\n",
    "        for op_name in weights_dict:\n",
    "\n",
    "            # Check if layer should be trained from scratch\n",
    "            if op_name not in self.SKIP_LAYER:\n",
    "\n",
    "                with tf.variable_scope(op_name, reuse=True):\n",
    "\n",
    "                    # Assign weights/biases to their corresponding tf variable\n",
    "                    for data in weights_dict[op_name]:\n",
    "\n",
    "                        # Biases\n",
    "                        if len(data.shape) == 1:\n",
    "                            var = tf.get_variable('biases', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "                        # Weights\n",
    "                        else:\n",
    "                            var = tf.get_variable('weights', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "\n",
    "def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n",
    "         padding='SAME', groups=1):\n",
    "    \n",
    "    # Get number of input channels\n",
    "    input_channels = int(x.get_shape()[-1])\n",
    "\n",
    "    # Create lambda function for the convolution\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k,\n",
    "                                         strides=[1, stride_y, stride_x, 1],\n",
    "                                         padding=padding)\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # Create tf variables for the weights and biases of the conv layer\n",
    "        weights = tf.get_variable('weights', shape=[filter_height,\n",
    "                                                    filter_width,\n",
    "                                                    input_channels/groups,\n",
    "                                                    num_filters])\n",
    "        biases = tf.get_variable('biases', shape=[num_filters])\n",
    "\n",
    "    if groups == 1:\n",
    "        conv = convolve(x, weights)\n",
    "\n",
    "    # In the cases of multiple groups, split inputs & weights and\n",
    "    else:\n",
    "        # Split input and weights and convolve them separately\n",
    "        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\n",
    "        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\n",
    "                                 value=weights)\n",
    "        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\n",
    "\n",
    "        # Concat the convolved output together again\n",
    "        conv = tf.concat(axis=3, values=output_groups)\n",
    "\n",
    "    # Add biases\n",
    "    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\n",
    "\n",
    "    # Apply relu function\n",
    "    relu = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    return relu\n",
    "\n",
    "\n",
    "def fc(x, num_in, num_out, name, relu=True):\n",
    "    \"\"\"Create a fully connected layer.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "\n",
    "        # Create tf variables for the weights and biases\n",
    "        weights = tf.get_variable('weights', shape=[num_in, num_out],\n",
    "                                  trainable=True)\n",
    "        biases = tf.get_variable('biases', [num_out], trainable=True)\n",
    "\n",
    "        # Matrix multiply weights and inputs and add bias\n",
    "        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
    "\n",
    "    if relu:\n",
    "        # Apply ReLu non linearity\n",
    "        relu = tf.nn.relu(act)\n",
    "        return relu\n",
    "    else:\n",
    "        return act\n",
    "\n",
    "\n",
    "def max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\n",
    "             padding='SAME'):\n",
    "   \n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n",
    "                          strides=[1, stride_y, stride_x, 1],\n",
    "                          padding=padding, name=name)\n",
    "\n",
    "\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
    "   \n",
    "    return tf.nn.local_response_normalization(x, depth_radius=radius,\n",
    "                                              alpha=alpha, beta=beta,\n",
    "                                              bias=bias, name=name)\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "   \n",
    "    return tf.nn.dropout(x, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 17  2 15 10 11 25  9 19 13 27  0 16  6 12  4 22 18  3 28 21  5  1 24\n",
      " 30  8 23 26  7 20 14] ['tape_dispenser', 'mug', 'bike_helmet', 'monitor', 'headphones', 'keyboard', 'ruler', 'file_cabinet', 'pen', 'letter_tray', 'speaker', 'back_pack', 'mouse', 'desk_chair', 'laptop_computer', 'bottle', 'projector', 'paper_notebook', 'bookcase', 'stapler', 'printer', 'calculator', 'bike', 'ring_binder', 'trash_can', 'desktop_computer', 'punchers', 'scissors', 'desk_lamp', 'phone', 'mobile_phone']\n",
      "[29 17  2 15 10 11 25  9 19 13 27  0 16  6 12  4 22 18  3 28 21  5  1 24\n",
      " 30  8 23 26  7 20 14] ['tape_dispenser', 'mug', 'bike_helmet', 'monitor', 'headphones', 'keyboard', 'ruler', 'file_cabinet', 'pen', 'letter_tray', 'speaker', 'back_pack', 'mouse', 'desk_chair', 'laptop_computer', 'bottle', 'projector', 'paper_notebook', 'bookcase', 'stapler', 'printer', 'calculator', 'bike', 'ring_binder', 'trash_can', 'desktop_computer', 'punchers', 'scissors', 'desk_lamp', 'phone', 'mobile_phone']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const:0\", shape=(3,), dtype=float32) must be from the same graph as Tensor(\"TensorSliceDataset:0\", shape=(), dtype=variant, device=/device:CPU:0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-95dd9e3e42c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Ops for initializing the two different iterators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtraining_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mvalidation_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mmake_initializer\u001b[0;34m(self, dataset, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m       return gen_dataset_ops.make_iterator(\n\u001b[0;32m--> 297\u001b[0;31m           dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1394\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m     return gen_dataset_ops.batch_dataset(\n\u001b[0;32m-> 1396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         output_shapes=nest.flatten(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1297\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     return gen_dataset_ops.shuffle_dataset(\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1847\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m     return gen_dataset_ops.prefetch_dataset(\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         output_shapes=nest.flatten(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1636\u001b[0m             sparse.as_dense_types(self.output_types, self.output_classes)),\n\u001b[1;32m   1637\u001b[0m         output_shapes=nest.flatten(\n\u001b[0;32m-> 1638\u001b[0;31m             sparse.as_dense_shapes(self.output_shapes, self.output_classes)))\n\u001b[0m\u001b[1;32m   1639\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0mother_arguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m         output_shapes=output_shapes, name=name)\n\u001b[0m\u001b[1;32m   1367\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5053\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5054\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5055\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5056\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5057\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   4989\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4990\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[0;32m-> 4991\u001b[0;31m                                                                 original_item))\n\u001b[0m\u001b[1;32m   4992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const:0\", shape=(3,), dtype=float32) must be from the same graph as Tensor(\"TensorSliceDataset:0\", shape=(), dtype=variant, device=/device:CPU:0)."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Learning params\n",
    "learning_rate = 0.000002\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "case='amazon'\n",
    "\n",
    "# Network params\n",
    "dropout_rate = 0.2\n",
    "num_classes = 2\n",
    "train_layers = ['fc8', 'fc7', 'fc6']\n",
    "\n",
    "# How often we want to write the tf.summary data to disk\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/home/ubuntu/tmp/finetune_alexnet/tensorboard\"\n",
    "checkpoint_path = \"/home/ubuntu/tmp/finetune_alexnet/check\"\n",
    "\n",
    "\"\"\"\n",
    "Main Part of the finetuning Script.\n",
    "\"\"\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "\n",
    "# Place data loading and preprocessing on the cpu\n",
    "with tf.device('/cpu:0'):\n",
    "    tr_data = ImageDataGenerator(case,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=True)\n",
    "    val_data = ImageDataGenerator(case,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                       tr_data.data.output_shapes)\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "# Ops for initializing the two different iterators\n",
    "training_init_op = iterator.make_initializer(tr_data.data)\n",
    "validation_init_op = iterator.make_initializer(val_data.data)\n",
    "\n",
    "# TF placeholder for graph input and output\n",
    "x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Initialize model\n",
    "model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "# Link variable to model output\n",
    "score = model.fc8\n",
    "\n",
    "# List of trainable variables of the layers we want to train\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=score,\n",
    "                                                                  labels=y))\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    # Get gradients of all trainable variables\n",
    "    gradients = tf.gradients(loss, var_list)\n",
    "    gradients = list(zip(gradients, var_list))\n",
    "\n",
    "    # Create optimizer and apply gradient descent to the trainable variables\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "# Add gradients to summary\n",
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "# Add the variables we train to the summary\n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize the FileWriter\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = int(np.floor(tr_data.data_size/batch_size))\n",
    "val_batches_per_epoch = int(np.floor(val_data.data_size / batch_size))\n",
    "\n",
    "# Start Tensorflow session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    # Load the pretrained weights into the non-trainable layer\n",
    "    model.load_initial_weights(sess)\n",
    "\n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "\n",
    "    # Loop over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch,\n",
    "                                          y: label_batch,\n",
    "                                          keep_prob: dropout_rate})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch,\n",
    "                                                        keep_prob: 1.})\n",
    "\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        for _ in range(val_batches_per_epoch):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch,\n",
    "                                                keep_prob: 1.})\n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "\n",
    "        # save checkpoint of the model\n",
    "        checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                       'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "        print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                       checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
